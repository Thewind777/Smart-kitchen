import os
import sqlite3
import requests
import json
from dotenv import load_dotenv

# Load credentials
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
SQLITE_DB = "data/prices.db"

if not SUPABASE_URL or not SUPABASE_KEY:
    print("❌ Error: Missing Supabase Credentials in .env")
    exit(1)

HEADERS = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json"
}

def execute_sql_supabase(sql_query):
    """
    Executes raw SQL via Supabase SQL endpoint (pgmeta).
    Note: Supabase standard REST API doesn't support raw SQL DDL easily without the pgmeta extension or SQL editor.
    However, for this migration script, we will use the REST API 'RPC' (Stored Procedure) trick OR just print the SQL 
    for the user to run in the Supabase Dashboard SQL Editor if we can't connect directly via psycopg2.
    
    BUT, we can try to use the REST API to insert data if the tables exist.
    
    Let's try to AUTOMATE table creation if possible.
    Since we don't have psycopg2 guaranteed, we will rely on a printed SQL Schema output
    that the user (or I via the dashboard interaction, if I could) would run.
    
    WAIT! I can use `requests` to call the `rpc` endpoint if I create a `exec_sql` function in the DB.
    
    PLAN B: Generate a `schema.sql` file.
    PLAN C (Data Migration): Read SQLite, Loop, POST to Supabase REST API.
    """
    pass

def generate_schema_file():
    print("Generating schema.sql for Supabase...")
    schema = """
    -- Enable Vector Extension for future AI features
    create extension if not exists vector;

    -- Products Table
    create table if not exists products (
        id bigint generated by default as identity primary key,
        name text not null,
        price double precision not null,
        currency text default 'EUR',
        unit text,
        store text not null,
        original_url text,
        image_url text,
        calories_100g integer,
        nutriscore text,
        last_enriched timestamp with time zone default timezone('utc'::text, now()),
        created_at timestamp with time zone default timezone('utc'::text, now())
    );

    -- Recipes Table
    create table if not exists recipes (
        id text primary key, -- TheMealDB IDs are strings
        title text not null,
        image_url text,
        instructions text,
        prep_time integer,
        serving_size integer,
        created_at timestamp with time zone default timezone('utc'::text, now())
    );

    -- Recipe Ingredients Link
    create table if not exists recipe_ingredients (
        id bigint generated by default as identity primary key,
        recipe_id text references recipes(id) on delete cascade,
        ingredient_name text not null,
        quantity text
    );

    -- Indexing for performance
    create index if not exists idx_products_name on products using gin(to_tsvector('english', name));
    create index if not exists idx_products_store on products(store);
    """
    
    with open("supabase_schema.sql", "w") as f:
        f.write(schema)
    print("✅ Created 'supabase_schema.sql'. Please run this in your Supabase SQL Editor.")

def migrate_data():
    print("Migrating Data from SQLite to Supabase...")
    if not os.path.exists(SQLITE_DB):
        print("No local DB found. Skipping data migration.")
        return

    conn = sqlite3.connect(SQLITE_DB)
    conn.row_factory = sqlite3.Row
    c = conn.cursor()

    # 1. Migrate Products
    try:
        rows = c.execute("SELECT * FROM products").fetchall()
        print(f"Found {len(rows)} products to migrate.")
        
        products_batch = []
        for row in rows:
            # Map SQLite row to Postgres Dict
            product = {
                "name": row["name"],
                "price": row["price"],
                "currency": row["currency"],
                "unit": row["unit"],
                "store": row["store"],
                "original_url": row["original_url"],
                "image_url": row["image_url"] if "image_url" in row.keys() else None,
                # Safe fallback for new columns if they exist in sqlite
                "nutriscore": row["nutriscore"] if "nutriscore" in row.keys() else None
            }
            products_batch.append(product)
            
            if len(products_batch) >= 100:
                _push_batch("products", products_batch)
                products_batch = []
        
        if products_batch:
            _push_batch("products", products_batch)
            
    except Exception as e:
        print(f"Error migrating products: {e}")

    # 2. Migrate Recipes
    try:
        rows = c.execute("SELECT * FROM recipes").fetchall()
        print(f"Found {len(rows)} recipes to migrate.")
        
        recipes_batch = []
        for row in rows:
            recipe = {
                "id": row["id"],
                "title": row["title"],
                "image_url": row["image_url"],
                "instructions": row["instructions"],
                "prep_time": row["prep_time"],
                "serving_size": row["serving_size"]
            }
            recipes_batch.append(recipe)
        
        if recipes_batch:
            _push_batch("recipes", recipes_batch)
            
    except Exception as e:
        print(f"Error migrating recipes: {e}")
        
    # 3. Migrate Ingredients
    try:
        rows = c.execute("SELECT * FROM recipe_ingredients").fetchall()
        print(f"Found {len(rows)} ingredients to migrate.")
        
        ing_batch = []
        for row in rows:
            ing = {
                "recipe_id": row["recipe_id"],
                "ingredient_name": row["ingredient_name"],
                "quantity": row["quantity"]
            }
            ing_batch.append(ing)
            
            if len(ing_batch) >= 100:
                _push_batch("recipe_ingredients", ing_batch)
                ing_batch = []
        
        if ing_batch:
            _push_batch("recipe_ingredients", ing_batch)
            
    except Exception as e:
        print(f"Error migrating ingredients: {e}")

    conn.close()

def _push_batch(table, data):
    url = f"{SUPABASE_URL}/rest/v1/{table}"
    try:
        # Prefer=resolution=ignore-duplicates allows us to re-run safely
        headers = HEADERS.copy()
        headers["Prefer"] = "resolution=ignore-duplicates"
        
        response = requests.post(url, headers=headers, json=data)
        if response.status_code not in [200, 201]:
            print(f"⚠️ Batch Upload Failed for {table}: {response.text}")
        else:
            print(f"✅ Uploaded {len(data)} rows to {table}.")
    except Exception as e:
        print(f"Network error pushing to {table}: {e}")

if __name__ == "__main__":
    generate_schema_file()
    migrate_data()
